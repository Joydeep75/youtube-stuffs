{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudarshan-koirala/youtube-stuffs/blob/main/llamaindex/Llama_index_pinecone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4eK-unQlKTF"
      },
      "source": [
        "# Llama-Index with Pinecone\n",
        "\n",
        "In this notebook, we will demo how to use the `llama-index` (previously GPT-index) library with Pinecone for semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YenjP38jtRdE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llama-index datasets pinecone-client openai transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjeiHqOIoGnI"
      },
      "source": [
        "We can go ahead and load the [SQuAD dataset from Huggingface](https://huggingface.co/datasets/squad), which contains questions and answer pairs from Wikipedia articles.\n",
        "\n",
        "[Datasets](https://github.com/huggingface/datasets) github repo.\n",
        "\n",
        "We'll then convert the dataset into a pandas DataFrame and keep only the unique 'context' fields, which are the text passages that the questions are based on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlKkc8Jbqiin"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset('squad', split='train')\n",
        "data = data.to_pandas()[['id', 'context', 'title']]\n",
        "data.drop_duplicates(subset='context', keep='first', inplace=True)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT6Z5gW4oTkG"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK90FFhf42hd"
      },
      "source": [
        "The following code transforms our DataFrame into a list of Document objects, ready for indexing with llama_index.\n",
        "\n",
        "Each document contains the follwing:\n",
        "- text passage\n",
        "- a unique id\n",
        "- an extra field for the article title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXOYMGfLtO-Z"
      },
      "outputs": [],
      "source": [
        "from llama_index import Document\n",
        "\n",
        "docs = []\n",
        "\n",
        "for i, row in data.iterrows():\n",
        "    docs.append(Document(\n",
        "        text=row['context'],\n",
        "        doc_id=row['id'],\n",
        "        extra_info={'title': row['title']}\n",
        "    ))\n",
        "print(len(docs))\n",
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = docs[:100]\n",
        "len(documents)"
      ],
      "metadata": {
        "id": "poj6joSf-2gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQrAcAdZ1fS9"
      },
      "source": [
        "### Indexing in Pinecone\n",
        "\n",
        "[Pinecone](https://www.pinecone.io/) is a managed vector database service designed for machine learning applications. We're using it in this context to store and retrieve embeddings generated by our language model, enabling efficient and scalable semantic similarity-based search.\n",
        "\n",
        "Get the relevant API key and environment that we [get for **free** in the console](https://app.pinecone.io/), then create a new index.\n",
        "\n",
        "The index has a dimension of 1536 and uses cosine similarity, which is the recommended metric for comparing vectors produced by the `text-embedding-ada-002` model we'll be using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SjIHPZgy8Zr"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "\n",
        "# find API key in console at app.pinecone.io\n",
        "os.environ['PINECONE_API_KEY'] = 'PINECONE_API_KEY'\n",
        "# environment is found next to API key in the console\n",
        "os.environ['PINECONE_ENVIRONMENT'] = 'asia-southeast1-gcp'\n",
        "\n",
        "# initialize connection to pinecone\n",
        "pinecone.init(\n",
        "    api_key=os.environ['PINECONE_API_KEY'],\n",
        "    environment=os.environ['PINECONE_ENVIRONMENT']\n",
        ")\n",
        "\n",
        "# create the index if it does not exist already\n",
        "index_name = 'llama-index-pinecone'\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,\n",
        "        metric='cosine'\n",
        "    )\n",
        "\n",
        "# connect to the index\n",
        "pinecone_index = pinecone.Index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4KQXwLh645t"
      },
      "source": [
        "Here, we're initializing a `PineconeVectorStore` with our previously created Pinecone index. This object will serve as the storage and retrieval interface for our document embeddings in Pinecone's vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO4eAvSn4t-R"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores import PineconeVectorStore\n",
        "\n",
        "# we can select a namespace (acts as a partition in an index)\n",
        "namespace = '' # default namespace\n",
        "\n",
        "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qotocu00ZjfF"
      },
      "source": [
        "Next we initialize the `GPTVectorStoreIndex` with our list of `Document` objects, using the `PineconeVectorStore` as storage and `OpenAIEmbedding` model for embeddings.\n",
        "\n",
        "`StorageContext` is used to configure the storage setup, and `ServiceContext` sets up the embedding model. The `GPTVectorStoreIndex` handles the indexing and querying process, making use of the provided storage and service contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V64_Y5FZqJ8w"
      },
      "outputs": [],
      "source": [
        "from llama_index import GPTVectorStoreIndex, StorageContext, ServiceContext\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# setup our storage (vector db)\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=vector_store\n",
        ")\n",
        "\n",
        "import os\n",
        "# https://platform.openai.com/account/api-keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "\n",
        "# setup the index/query process, ie the embedding model (and completion if used)\n",
        "embed_model = OpenAIEmbedding(model='text-embedding-ada-002', embed_batch_size=100)\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
        "\n",
        "index = GPTVectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context,\n",
        "    service_context=service_context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7veOHn3Zyv1"
      },
      "source": [
        "Finally we can build a query engine from the `index` we build and use this engine to perform a query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4loa7g_y5x2v"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine()\n",
        "res = query_engine.query(\"In what year was the college of engineering established at the University of Notre Dame?\")\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.context[20]"
      ],
      "metadata": {
        "id": "e7dC3VbgEZlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "res = query_engine.query(\"When was the First Year of Studies program established at the University of Notre Dame has?\")\n",
        "print(res)"
      ],
      "metadata": {
        "id": "GvY7c7CrDJqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete the `index` if not necessary."
      ],
      "metadata": {
        "id": "dWKsK8PVCpUN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z89CVJMGaJ4j"
      },
      "outputs": [],
      "source": [
        "pinecone.delete_index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to learn more into pinecone, you can visit the [pinecone github examples](https://github.com/pinecone-io/examples/tree/master)"
      ],
      "metadata": {
        "id": "vcOooYYqFAK9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWtX3Nxl5V71"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}